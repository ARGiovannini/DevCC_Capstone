{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Poker Tournament scheduler**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Main Idea:*\n",
    "Poker proffesionals effectively run a business in which their decision making skills are the product, and applied over time will allow them to be proffitable. Many people are actively using data science and game theory to get better at playing the game, and increasing profit based on their decision in the game. However, one of the most important decisions comes before you ever sit down to play, deciding which game to play in. Cash games are more difficult to obtain any data on because, as the name implies, the games operate in cash, and aren't easily tracked. Poker tournaments however will be completely tracked, and lots of data will be available for them.\n",
    "\n",
    "With all of this in mind, the following objective is to find all the information that is available on poker tournaments around the world, and create a dashboard that can be interactively filtered so that an end user can develop a poker tournamnet scheduler for themeselves."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Steps*\n",
    "\n",
    "1. Find and scrape as much data as I can on poker tournaments\n",
    "2. Clean all available data\n",
    "3. Brainstorm ways to classify tournaments as high or low value, and sort tournaments for end user filtering\n",
    "4. Bring data into Tableau and create a dashboard that is easily navigable\n",
    "5. Allow end users to export a CSV of their tournament schedule for the next 365 days"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Set up imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Create CSV to store data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"Tournaments_data_3.csv\", \"w\")\n",
    "writer = csv.writer(file)\n",
    "writer.writerow([\"Unique ID\", \"Date\", \"Start time\", \"location\", \"Venue\", \"Game type\", \"Total buy in\", \"Prize pool\", \"rake\", \"Guarantee\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_driver = Service('/Users/alexgiovannini/Desktop/DevCC/Week 19 - 23 capstone/Project ideas/Poker/chromedriver')\n",
    "scraper = webdriver.Chrome(service=browser_driver)\n",
    "wait = WebDriverWait(scraper, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Get location elements from \"https://www.pokeratlas.com/areas\" and store them as text in \"locations_text_list.\" Then set up the unique ID and \"locations_scraped\" list to be used later*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.get(\"https://www.pokeratlas.com/areas\")\n",
    "\n",
    "try:\n",
    "    popup = scraper.find_element(By.CLASS_NAME, \"modal-close\")\n",
    "    wait.until(EC.element_to_be_clickable(popup))\n",
    "    popup.click()\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "except StaleElementReferenceException:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    cookies = scraper.find_element(By.XPATH, \"//*[@id='modal-gdpr-cookies']/button\")\n",
    "    cookies.click()\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "except StaleElementReferenceException:\n",
    "    pass\n",
    "\n",
    "US_element = scraper.find_element(By.CLASS_NAME, \"country\")\n",
    "locations_list = US_element.find_elements(By.TAG_NAME, \"li\")\n",
    "locations_text_list = []\n",
    "for location in locations_list:\n",
    "    locations_text_list.append(location.text)\n",
    "unique_id = 0\n",
    "locations_scraped = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pokeratlas.com scraper version 1.\n",
    "### Run this script to scrape tournaments using link text iterating through \"locations_text_list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scraper.get(\"https://www.pokeratlas.com/areas\")\n",
    "file = open(\"Tournaments_data_5.csv\", \"w\")\n",
    "writer = csv.writer(file)\n",
    "writer.writerow([\"Unique ID\", \"Date\", \"Start time\", \"location\", \n",
    "\"Venue\", \"Game type\", \"Total buy in\", \"Prize pool\", \"rake\", \"Guarantee\"])\n",
    "for location in locations_text_list:\n",
    "    try:\n",
    "        popup = scraper.find_element(By.CLASS_NAME, \"modal-close\")\n",
    "        wait.until(EC.element_to_be_clickable(popup))\n",
    "        popup.click()\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    except StaleElementReferenceException:\n",
    "        pass\n",
    "\n",
    "    if location not in locations_scraped:\n",
    "        scraper.find_element(By.LINK_TEXT, location).click()\n",
    "        locations_scraped.append(location)\n",
    "\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            popup = scraper.find_element(By.CLASS_NAME, \"modal-close\")\n",
    "            wait.until(EC.element_to_be_clickable(popup))\n",
    "            popup.click()\n",
    "        except TimeoutException:\n",
    "            pass\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        except StaleElementReferenceException:\n",
    "            pass\n",
    "\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            see_all_tournaments = scraper.find_element(By.LINK_TEXT, 'See All Tournaments')\n",
    "            see_all_tournaments.click()\n",
    "        except NoSuchElementException:\n",
    "            scraper.back()\n",
    "            continue\n",
    "\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            popup = scraper.find_element(By.CLASS_NAME, \"modal-close\")\n",
    "            popup.click()\n",
    "        except ElementNotInteractableException:\n",
    "            pass\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        except StaleElementReferenceException:\n",
    "            pass\n",
    "\n",
    "        scroll_pause_time = 1\n",
    "\n",
    "        screen_height = scraper.execute_script(\"return window.screen.height;\")\n",
    "        i = 1\n",
    "\n",
    "        while True:\n",
    "            scraper.execute_script(\"window.scrollTo(0, {screen_height} * {i});\".format(screen_height=screen_height, i=i))\n",
    "            i += 1\n",
    "            time.sleep(scroll_pause_time)\n",
    "            scroll_height = scraper.execute_script(\"return document.body.scrollHeight;\")\n",
    "            if (screen_height * i > scroll_height):\n",
    "                break\n",
    "\n",
    "        tournaments = scraper.find_elements(By.CLASS_NAME, \"tournament-item\")\n",
    "\n",
    "        for tournament in tournaments:\n",
    "            time.sleep(2)\n",
    "            if (\"warning\" in tournament.get_attribute(\"class\")):\n",
    "                continue\n",
    "            else:\n",
    "                venue = tournament.find_element(By.CLASS_NAME, \"venue-name\").text\n",
    "                date = tournament.find_element(By.CLASS_NAME, \"date\").text\n",
    "                start_time = tournament.find_element(By.CLASS_NAME, \"time\").text\n",
    "                game_type = tournament.find_element(By.CLASS_NAME, \"type\").find_element(By.TAG_NAME, \"span\").text\n",
    "                guarantee = tournament.find_element(By.CLASS_NAME, \"name\").text\n",
    "                tournament.click()\n",
    "                time.sleep(1)\n",
    "                buy_in_details = scraper.find_element(By.CLASS_NAME, \"buy-in-details\")\n",
    "                buy_in_list = buy_in_details.find_elements(By.CSS_SELECTOR, \"dd\")\n",
    "                if len(buy_in_list) < 3:\n",
    "                    total_buy_in = \"day 2\"\n",
    "                else:\n",
    "                    total_buy_in = buy_in_list[0].text\n",
    "                    prize_pool = buy_in_list[1].text\n",
    "                    rake = buy_in_list[2].text\n",
    "                scraper.back()\n",
    "                writer.writerow([unique_id, date, start_time, location, venue, game_type, total_buy_in, prize_pool, rake, guarantee])\n",
    "                unique_id += 1\n",
    "        scraper.back()\n",
    "        scraper.back()\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script ran for 319 minutes before I restarted it. it seemed to have hit the scraper.back() to many times and ended up at the start page.\n",
    "\n",
    "It got stuck at line 3917.\n",
    "\n",
    "The writer isn't writing because the file is closed. I am going to open a new file, \"tournaments_data_4\" and continue writing from there.\n",
    "\n",
    "Everytime I ran the script, it added a new location to the \"locations_scraped\", and then attempted to scrape it. once the error occured and none had been scraped, I had to \".pop()\" 7 times in order to return to the next location. because of this, I may be missing some data from  \"North and South Dakota\" past 2/10.\n",
    "\n",
    "The script ran for an hour before running into an error. I also didn't update it to open 4 instead of three. That one hour was only las vegas. So i then popped las vegas out and will re run again.\n",
    "\n",
    "I got another error on 3/3 in las vegas, So any tournaments past march 3 in vegas are missing.\n",
    "\n",
    "I also never reset the unique ID after running it a few times, so I will need to manually adjust the unique ID's when I combine the various CSV files.\n",
    "\n",
    "The script has now finished. I will most likely create a version 2.0 that has the following features:\n",
    "- Is faster\n",
    "- Scrapes Canada as well\n",
    "- Scrapes other countries if possible\n",
    "- automatically restarts if encounters an error. If it needs to restart, it will need to pop the last \"locations_scraped\" to get that data again\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (v3.10.4:9d38120e33, Mar 23 2022, 17:29:05) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
